{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e572682",
   "metadata": {},
   "source": [
    "# 01_data_preprocessing\n",
    "\n",
    "이 노트북은 원본 CSV(앱 사용/설문/수면)를 uid×week 단위로 병합해 분석 테이블을 생성합니다. 설문 점수(PHQ-9/GAD-7/Stress) 산출, 카테고리 피벗, 수면 지표 집계, 결측 대치, 초→시간 파생 컬럼 생성 후 저장합니다. (데이터 비공개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e090c",
   "metadata": {},
   "source": [
    "셀 1 — 설정/임포트/경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "899a5cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/biot/github/.venv/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: packaging in /home/biot/github/.venv/lib/python3.12/site-packages (25.0)\n",
      "Requirement already satisfied: setuptools in /home/biot/github/.venv/lib/python3.12/site-packages (80.9.0)\n",
      "Requirement already satisfied: pandas in /home/biot/github/.venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyarrow in /home/biot/github/.venv/lib/python3.12/site-packages (21.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /home/biot/github/.venv/lib/python3.12/site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/biot/github/.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/biot/github/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/biot/github/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/biot/github/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/biot/github/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# TL;DR: CSV 로드 → 점수화/피벗/집계/조인/결측대치 → *_hours 생성 → Parquet 저장\n",
    "import sys, os\n",
    "!{sys.executable} -m pip install -U pyspark packaging setuptools pandas pyarrow\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, split, expr, sum as spark_sum\n",
    ")\n",
    "import ast\n",
    "import pathlib\n",
    "\n",
    "# 입력 경로 (로컬 환경에 맞게 조정)\n",
    "APP_USAGE_CSV    = \"./../all_data/filtering_complete_app_usage.csv\"\n",
    "RESPONSE_CSV     = \"./../all_data/response_week_mapping_adjusted.csv\"\n",
    "SLEEP_CSV        = \"./../all_data/sleep_week_mapped.csv\"\n",
    "SLEEP_DIARY_CSV  = \"./../all_data/sleep_diary_week_mapped.csv\"\n",
    "\n",
    "# 출력 경로1\n",
    "pathlib.Path(\"results/tables\").mkdir(parents=True, exist_ok=True)\n",
    "PARQUET_OUT_DIR = \"results/tables/processed_weekly\"  # ← 확장자 없이 폴더명\n",
    "SAMPLE_OUT  = \"results/tables/processed_weekly_sample.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97afd2",
   "metadata": {},
   "source": [
    "셀 2 — Spark 세션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "794a08fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: None\n",
      "java in PATH: /usr/bin/java\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "print(\"java in PATH:\", shutil.which(\"java\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "025f3579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 03:58:28 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/08/21 03:58:28 ERROR Configuration: error parsing conf core-default.xml\n",
      "java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n",
      "\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n",
      "\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n",
      "\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n",
      "\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n",
      "\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n",
      "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n",
      "\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n",
      "\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n",
      "\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n",
      "\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n",
      "\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n",
      "\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/08/21 03:58:28 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.RuntimeException: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n",
      "\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n",
      "\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n",
      "\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n",
      "\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n",
      "\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n",
      "\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n",
      "\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n",
      "\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n",
      "\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n",
      "\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n",
      "\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n",
      "\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n",
      "\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n",
      "\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n",
      "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n",
      "\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n",
      "\t... 27 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 새 세션 생성\u001b[39;00m\n\u001b[32m      2\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAppUsageMentalHealth-Preprocess\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.session.timeZone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUTC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/context.py:203\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/context.py:296\u001b[39m, in \u001b[36m_do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/context.py:421\u001b[39m, in \u001b[36m_initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1587\u001b[39m, in \u001b[36m__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1583\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaClass(\n\u001b[32m   1584\u001b[39m             \u001b[38;5;28mself\u001b[39m._fqn + \u001b[33m\"\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\"\u001b[39m + name, \u001b[38;5;28mself\u001b[39m._gateway_client)\n\u001b[32m   1585\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1586\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m get_return_value(\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m             answer, \u001b[38;5;28mself\u001b[39m._gateway_client, \u001b[38;5;28mself\u001b[39m._fqn, name)\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m   1590\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m does not exist in the JVM\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m._fqn, name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mdeco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1412)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:512)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:443)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:428)\n\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:99)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:262)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "# 새 세션 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AppUsageMentalHealth-Preprocess\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c491de",
   "metadata": {},
   "source": [
    "셀 3 — 데이터 로드(+타입)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "941e8fb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o835.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1180)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1213)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1228)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/spark-hive_2.12-3.5.1.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1172)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m app_usage_df    = (\u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPP_USAGE_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m                    .withColumn(\u001b[33m\"\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mdouble\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m      3\u001b[39m raw_response_df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m).csv(RESPONSE_CSV)\n\u001b[32m      4\u001b[39m sleep_df        = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m).csv(SLEEP_CSV)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mdeco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o835.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1180)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1213)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1228)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/spark-hive_2.12-3.5.1.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1172)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "app_usage_df    = (spark.read.option(\"header\", True).csv(APP_USAGE_CSV)\n",
    "                   .withColumn(\"duration\", col(\"duration\").cast(\"double\")))\n",
    "raw_response_df = spark.read.option(\"header\", True).csv(RESPONSE_CSV)\n",
    "sleep_df        = spark.read.option(\"header\", True).csv(SLEEP_CSV)\n",
    "sleep_diary_df  = spark.read.option(\"header\", True).csv(SLEEP_DIARY_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c475f1",
   "metadata": {},
   "source": [
    "셀 4 — 설문 점수 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(answer_list, weight_list):\n",
    "    if not isinstance(answer_list, list):\n",
    "        return 0\n",
    "    for i, ans in enumerate(answer_list):\n",
    "        if ans:\n",
    "            return weight_list[i]\n",
    "    return 0\n",
    "\n",
    "def parse_questionnaire(q_list_str, weight_list, filter_options=None):\n",
    "    total = 0\n",
    "    try:\n",
    "        q_list = ast.literal_eval(q_list_str)\n",
    "        for q in q_list:\n",
    "            if filter_options and q.get('options') != filter_options:\n",
    "                continue\n",
    "            total += calc_score(q.get('answers'), weight_list)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f98ee",
   "metadata": {},
   "source": [
    "셀 5 — 설문 점수 DataFrame 생성 (RDD→DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----+----------+----------+------------+\n",
      "|uid                         |week|PHQ9_score|GAD7_score|Stress_score|\n",
      "+----------------------------+----+----------+----------+------------+\n",
      "|05U1A5bmcnUUycZ6SYAdqhuu3ck2|0   |1         |0         |2           |\n",
      "|05U1A5bmcnUUycZ6SYAdqhuu3ck2|1   |0         |0         |0           |\n",
      "|05U1A5bmcnUUycZ6SYAdqhuu3ck2|2   |1         |0         |2           |\n",
      "|05U1A5bmcnUUycZ6SYAdqhuu3ck2|3   |0         |0         |0           |\n",
      "|05U1A5bmcnUUycZ6SYAdqhuu3ck2|4   |0         |0         |2           |\n",
      "+----------------------------+----+----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "response_rdd = raw_response_df.rdd.map(lambda r: Row(\n",
    "    uid = r['uid'],\n",
    "    week = int(r['week']),\n",
    "    PHQ9_score = parse_questionnaire(r['PHQ-9'], [0,1,2,3]),\n",
    "    GAD7_score = parse_questionnaire(r['GAD-7'], [0,1,2,3]),\n",
    "    Stress_score = parse_questionnaire(\n",
    "        r['Stress Questionnaire'], [0,1,2,3,4],\n",
    "        filter_options=['전혀 그렇지 않다','약간 그렇다','웬만큼그렇다','상당히그렇다','아주 그렇다'])\n",
    "))\n",
    "response_df = spark.createDataFrame(response_rdd).dropDuplicates([\"uid\",\"week\"])\n",
    "response_df.orderBy(\"uid\",\"week\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c39038",
   "metadata": {},
   "source": [
    "셀 6 — 앱 사용 집계 → 카테고리 피벗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키 중복 방지\n",
    "app_usage_df = app_usage_df.dropDuplicates([\"uid\",\"week\",\"category\"])\n",
    "\n",
    "app_summary_df = (app_usage_df\n",
    "                  .groupBy(\"uid\",\"week\",\"category\")\n",
    "                  .agg(spark_sum(\"duration\").alias(\"duration\")))\n",
    "\n",
    "app_pivot_df = (app_summary_df\n",
    "                .groupBy(\"uid\",\"week\")\n",
    "                .pivot(\"category\")\n",
    "                .sum(\"duration\")\n",
    "                .na.fill(0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce37e2",
   "metadata": {},
   "source": [
    "셀 7 — 수면 집계 (HH:MM:SS → 분 변환 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meanConfidence 평균\n",
    "sleep_agg_df = (sleep_df\n",
    "    .filter(col(\"week\").isNotNull())\n",
    "    .withColumn(\"meanConfidence\", col(\"meanConfidence\").cast(\"double\"))\n",
    "    .groupBy(\"uid\",\"week\")\n",
    "    .agg(avg(\"meanConfidence\").alias(\"mean_confidence_sleep\"))\n",
    ").dropDuplicates([\"uid\",\"week\"])\n",
    "\n",
    "# 수면 다이어리: \"HH:MM:SS\" → 분\n",
    "sleep_diary_min = (sleep_diary_df\n",
    "    .filter(col(\"week\").isNotNull())\n",
    "    .withColumn(\"parts\", split(col(\"midawake_duration\"), \":\"))\n",
    "    .withColumn(\"midawake_duration_min\",\n",
    "                col(\"parts\").getItem(0).cast(\"double\")*60 +\n",
    "                col(\"parts\").getItem(1).cast(\"double\") +\n",
    "                col(\"parts\").getItem(2).cast(\"double\")/60.0)\n",
    ")\n",
    "sleep_diary_agg_df = (sleep_diary_min\n",
    "    .groupBy(\"uid\",\"week\")\n",
    "    .agg(avg(\"midawake_duration_min\").alias(\"midawake_duration_sleep\"))\n",
    ").dropDuplicates([\"uid\",\"week\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0740831",
   "metadata": {},
   "source": [
    "셀 8 — 조인 & 결측 대치 → *_hours 생성 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af2c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o817.parquet.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1180)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1213)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1228)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/spark-hive_2.12-3.5.1.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1172)\n\t... 36 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m final_df = final_df.dropDuplicates([\u001b[33m\"\u001b[39m\u001b[33muid\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mweek\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 저장\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mfinal_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARQUET_OUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m final_df.limit(\u001b[32m50\u001b[39m).toPandas().to_csv(SAMPLE_OUT, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Saved:\u001b[39m\u001b[33m\"\u001b[39m, PARQUET_OUT, \u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m, SAMPLE_OUT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1721\u001b[39m, in \u001b[36mparquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   1680\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1682\u001b[39m     path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1686\u001b[39m     **options: \u001b[33m\"\u001b[39m\u001b[33mOptionalPrimitiveType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1687\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Saves the contents of the :class:`DataFrame` to a data source.\u001b[39;00m\n\u001b[32m   1689\u001b[39m \n\u001b[32m   1690\u001b[39m \u001b[33;03m    The data source is specified by the ``format`` and a set of ``options``.\u001b[39;00m\n\u001b[32m   1691\u001b[39m \u001b[33;03m    If ``format`` is not specified, the default data source configured by\u001b[39;00m\n\u001b[32m   1692\u001b[39m \u001b[33;03m    ``spark.sql.sources.default`` will be used.\u001b[39;00m\n\u001b[32m   1693\u001b[39m \n\u001b[32m   1694\u001b[39m \u001b[33;03m    .. versionadded:: 1.4.0\u001b[39;00m\n\u001b[32m   1695\u001b[39m \n\u001b[32m   1696\u001b[39m \u001b[33;03m    .. versionchanged:: 3.4.0\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m        Supports Spark Connect.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \n\u001b[32m   1699\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   1700\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m   1701\u001b[39m \u001b[33;03m    path : str, optional\u001b[39;00m\n\u001b[32m   1702\u001b[39m \u001b[33;03m        the path in a Hadoop supported file system\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[33;03m    format : str, optional\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[33;03m        the format used to save\u001b[39;00m\n\u001b[32m   1705\u001b[39m \u001b[33;03m    mode : str, optional\u001b[39;00m\n\u001b[32m   1706\u001b[39m \u001b[33;03m        specifies the behavior of the save operation when data already exists.\u001b[39;00m\n\u001b[32m   1707\u001b[39m \n\u001b[32m   1708\u001b[39m \u001b[33;03m        * ``append``: Append contents of this :class:`DataFrame` to existing data.\u001b[39;00m\n\u001b[32m   1709\u001b[39m \u001b[33;03m        * ``overwrite``: Overwrite existing data.\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[33;03m        * ``ignore``: Silently ignore this operation if data already exists.\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[33;03m        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[33;03m            exists.\u001b[39;00m\n\u001b[32m   1713\u001b[39m \u001b[33;03m    partitionBy : list, optional\u001b[39;00m\n\u001b[32m   1714\u001b[39m \u001b[33;03m        names of partitioning columns\u001b[39;00m\n\u001b[32m   1715\u001b[39m \u001b[33;03m    **options : dict\u001b[39;00m\n\u001b[32m   1716\u001b[39m \u001b[33;03m        all other string options\u001b[39;00m\n\u001b[32m   1717\u001b[39m \n\u001b[32m   1718\u001b[39m \u001b[33;03m    Examples\u001b[39;00m\n\u001b[32m   1719\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m   1720\u001b[39m \u001b[33;03m    Write a DataFrame into a JSON file and read it back.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1721\u001b[39m \n\u001b[32m   1722\u001b[39m \u001b[33;03m    >>> import tempfile\u001b[39;00m\n\u001b[32m   1723\u001b[39m \u001b[33;03m    >>> with tempfile.TemporaryDirectory(prefix=\"save\") as d:\u001b[39;00m\n\u001b[32m   1724\u001b[39m \u001b[33;03m    ...     # Write a DataFrame into a JSON file\u001b[39;00m\n\u001b[32m   1725\u001b[39m \u001b[33;03m    ...     spark.createDataFrame(\u001b[39;00m\n\u001b[32m   1726\u001b[39m \u001b[33;03m    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\u001b[39;00m\n\u001b[32m   1727\u001b[39m \u001b[33;03m    ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\u001b[39;00m\n\u001b[32m   1728\u001b[39m \u001b[33;03m    ...\u001b[39;00m\n\u001b[32m   1729\u001b[39m \u001b[33;03m    ...     # Read the JSON file as a DataFrame.\u001b[39;00m\n\u001b[32m   1730\u001b[39m \u001b[33;03m    ...     spark.read.format('json').load(d).show()\u001b[39;00m\n\u001b[32m   1731\u001b[39m \u001b[33;03m    +---+------------+\u001b[39;00m\n\u001b[32m   1732\u001b[39m \u001b[33;03m    |age|        name|\u001b[39;00m\n\u001b[32m   1733\u001b[39m \u001b[33;03m    +---+------------+\u001b[39;00m\n\u001b[32m   1734\u001b[39m \u001b[33;03m    |100|Hyukjin Kwon|\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[33;03m    +---+------------+\u001b[39;00m\n\u001b[32m   1736\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode(mode).options(**options)\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partitionBy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mdeco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o817.parquet.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1180)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1213)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1228)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.NoSuchFileException: /home/biot/github/.venv/lib/python3.12/site-packages/pyspark/jars/spark-hive_2.12-3.5.1.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1851)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1432)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:717)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:251)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:180)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:346)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:117)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1172)\n\t... 36 more\n"
     ]
    }
   ],
   "source": [
    "# 조인\n",
    "final_df = (app_pivot_df\n",
    "            .join(response_df, [\"uid\",\"week\"], \"inner\")\n",
    "            .join(sleep_agg_df, [\"uid\",\"week\"], \"left\")\n",
    "            .join(sleep_diary_agg_df, [\"uid\",\"week\"], \"left\"))\n",
    "\n",
    "# 결측 평균 대치\n",
    "mean_conf = final_df.select(avg(\"mean_confidence_sleep\")).first()[0]\n",
    "mean_mid  = final_df.select(avg(\"midawake_duration_sleep\")).first()[0]\n",
    "final_df = final_df.na.fill({\n",
    "    \"mean_confidence_sleep\": float(mean_conf) if mean_conf is not None else 0.0,\n",
    "    \"midawake_duration_sleep\": float(mean_mid) if mean_mid is not None else 0.0\n",
    "})\n",
    "\n",
    "# 1) 점(.) 들어간 컬럼을 전부 언더바(_)로 변경\n",
    "for c in final_df.columns:\n",
    "    if \".\" in c:\n",
    "        final_df = final_df.withColumnRenamed(c, c.replace(\".\", \"_\"))\n",
    "\n",
    "# 2) 초→시간 파생\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sec_cols = [c for c in final_df.columns if c.startswith(\"AppCategory_\")]\n",
    "for c in sec_cols:\n",
    "    final_df = final_df.withColumn(f\"{c}_hours\", col(c) / 3600.0)\n",
    "\n",
    "# uid, week 중복 제거\n",
    "final_df = final_df.dropDuplicates([\"uid\",\"week\"])\n",
    "\n",
    "# 저장\n",
    "final_df.write.mode(\"overwrite\").parquet(PARQUET_OUT_DIR)\n",
    "final_df.limit(50).toPandas().to_csv(SAMPLE_OUT, index=False)\n",
    "\n",
    "print(\"✅ Saved:\", os.path.abspath(PARQUET_OUT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
      "PySpark: 3.5.1\n",
      "\u001b[33mWARNING: Package(s) not found: pyarrow, setuptools\u001b[0m\u001b[33m\n",
      "\u001b[0mName: packaging\n",
      "Version: 25.0\n",
      "Summary: Core utilities for Python packages\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Donald Stufft <donald@stufft.io>\n",
      "License: \n",
      "Location: /home/biot/github/.venv/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: ipykernel, matplotlib, statsmodels\n",
      "---\n",
      "Name: pandas\n",
      "Version: 2.2.2\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: /home/biot/github/.venv/lib/python3.12/site-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: seaborn, statsmodels\n"
     ]
    }
   ],
   "source": [
    "import sys, pyspark\n",
    "import platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"PySpark:\", pyspark.__version__)\n",
    "!{sys.executable} -m pip show setuptools packaging pandas pyarrow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
